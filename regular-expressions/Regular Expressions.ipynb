{
 "metadata": {
  "name": "",
  "signature": "sha256:12e5fb15aefbf1eeaae654f0d56caf9a623dfa9dafe5ec6b90433e0741872a47"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Regular Expressions playground\n",
      "\n",
      "I decided I wanted to write a regular expressions library. I don't know exactly why. It seemed cool, and I've never consciously worked with what I've heard as finite state automata. Here we go!\n",
      "\n",
      "\n",
      "# Initial thoughts\n",
      "\n",
      "Just a few stray thoughts spinning around:\n",
      "\n",
      " - Anchors seem real important for efficient matching\n",
      " - \"Compiling\" an expression will involve converting it into a more actionable representation/structure\n",
      "\n",
      "I'm gonna start simply by matching only a string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "\n",
      "\n",
      "# Th[is]? i. (a) test{2,6}\\. (Or|is|it?)\n",
      "\n",
      "T_EOF = -1       # Fake token to mark end of input. Makes parsing a bit easier.\n",
      "T_STRING = 0     # abcdef\\[\n",
      "T_DOT = 1        # .\n",
      "T_QUESTION = 2   # ?\n",
      "T_ASTERISK = 3   # *\n",
      "T_LBRACE = 4     # {\n",
      "T_RBRACE = 5     # }\n",
      "T_LBRACKET = 6   # [\n",
      "T_RBRACKET = 7   # ]\n",
      "T_LPAREN = 8     # (\n",
      "T_RPAREN = 9     # )\n",
      "T_COMMA = 10     # ,\n",
      "T_PIPE = 11      # |\n",
      "\n",
      "# For error reporting purposes\n",
      "TOKEN_NAMES = {\n",
      "    T_STRING: 'STRING',\n",
      "    T_DOT: 'DOT',\n",
      "    T_QUESTION: 'QUESTION',\n",
      "    T_ASTERISK: 'ASTERISK',\n",
      "    T_LBRACE: 'LBRACE',\n",
      "    T_RBRACE: 'RBRACE',\n",
      "    T_LBRACKET: 'LBRACKET',\n",
      "    T_RBRACKET: 'RBRACKET',\n",
      "    T_LPAREN: 'LPAREN',\n",
      "    T_RPAREN: 'RPAREN',\n",
      "    T_COMMA: 'COMMA',\n",
      "    T_PIPE: 'PIPE',\n",
      "}\n",
      "\n",
      "\n",
      "def _tokenize(s):\n",
      "    SINGLES = {\n",
      "        '.': T_DOT,\n",
      "        '?': T_QUESTION,\n",
      "        '*': T_ASTERISK,\n",
      "        '{': T_LBRACE,\n",
      "        '}': T_RBRACE,\n",
      "        '[': T_LBRACKET,\n",
      "        ']': T_RBRACKET,\n",
      "        '(': T_LPAREN,\n",
      "        ')': T_RPAREN,\n",
      "        ',': T_COMMA,\n",
      "        '|': T_PIPE,\n",
      "    }\n",
      "\n",
      "    Token = collections.namedtuple('Token', ['type', 'text', 'index'])\n",
      "\n",
      "    tokens = []\n",
      "    _literal = []\n",
      "    _literal_start = None\n",
      "    i = 0\n",
      "\n",
      "    def emit(type, text=None, index=None):\n",
      "        if text is None:\n",
      "            text = c\n",
      "        if index is None:\n",
      "            index = i\n",
      "        tokens.append(Token(type, text, index))\n",
      "\n",
      "    def literal():\n",
      "        nonlocal _literal_start\n",
      "        if _literal_start is None:\n",
      "            _literal_start = i\n",
      "        _literal.append(c)\n",
      "\n",
      "    def close_literal():\n",
      "        nonlocal _literal, _literal_start\n",
      "        if _literal:\n",
      "            text = ''.join(_literal)\n",
      "            emit(T_STRING, text, _literal_start)\n",
      "            _literal = []\n",
      "            _literal_start = None\n",
      "\n",
      "    while i < len(s):\n",
      "        c = s[i]\n",
      "        if c in SINGLES:\n",
      "            close_literal()\n",
      "            emit(SINGLES[c])\n",
      "        elif c == '\\\\':\n",
      "            # TODO: figure out what to really do here\n",
      "            if s[i+1] in SINGLES:\n",
      "                i += 1\n",
      "                literal()\n",
      "            else:\n",
      "                raise NotImplemented\n",
      "        else:\n",
      "            literal()\n",
      "\n",
      "        i += 1\n",
      "    close_literal()\n",
      "\n",
      "    emit(T_EOF, '')\n",
      "    return tokens\n",
      "\n",
      "\n",
      "class Match(object):\n",
      "    def __init__(self, source, text, position, groups=()):\n",
      "        self.source = source\n",
      "        self.text = text\n",
      "        self.start = position\n",
      "        self.end = self.start + len(self.text)\n",
      "        self._groups = (text,) + tuple(groups)\n",
      "\n",
      "    def group(self, n):\n",
      "        return self._groups[n]\n",
      "\n",
      "    def groups(self):\n",
      "        return self._groups\n",
      "\n",
      "\n",
      "S_STRING = 0\n",
      "S_GROUP = 1\n",
      "\n",
      "\n",
      "class Symbol(object):\n",
      "    def __init__(self, type_, text=None, children=None):\n",
      "        self.type = type_\n",
      "        self.text = text\n",
      "        if children is None:\n",
      "            children = ()\n",
      "        self.children = tuple(children)\n",
      "\n",
      "    def save(self, context, text):\n",
      "        \"\"\"Save the text matched for this symbol to the context\"\"\"\n",
      "        context[self] = text\n",
      "\n",
      "    def text(self, context):\n",
      "        if self in context:\n",
      "            return context[self]\n",
      "        elif self.children:\n",
      "            return ''.join(c.text(context) for c in self.children)\n",
      "\n",
      "\n",
      "class RegularExpression(object):\n",
      "    def __init__(self, source, symbols):\n",
      "        \"\"\"\n",
      "        :type source: str\n",
      "        :type symbols: list of Symbol\n",
      "        \"\"\"\n",
      "        self.source = source\n",
      "        self.symbols = symbols\n",
      "    \n",
      "    def __repr__(self):\n",
      "        return 'yre.compile(%r)' % self.source\n",
      "\n",
      "    def match(self, s):\n",
      "        context = {}\n",
      "        source = s\n",
      "        i = 0\n",
      "        t = 0\n",
      "        stack = []\n",
      "        symbols = self.symbols\n",
      "\n",
      "        while i < len(s) and t < len(self.symbols):\n",
      "            symbol = self.symbols[t]\n",
      "            s = s[i:]\n",
      "            if symbol.type == S_STRING:\n",
      "                if s.startswith(symbol.text):\n",
      "                    symbol.save(context, symbol.text)\n",
      "                    i += len(symbol.text)\n",
      "                    t += 1\n",
      "                    continue\n",
      "\n",
      "            return None\n",
      "        else:\n",
      "            text = source[:i+1]\n",
      "            return Match(source, text, 0)\n",
      "\n",
      "\n",
      "_cache = {}\n",
      "def compile(s):\n",
      "    if s in _cache:\n",
      "        return _cache[s]\n",
      "    \n",
      "    tokens = _tokenize(s)\n",
      "    i = 0\n",
      "    pieces = []  # Intermediate storage for children\n",
      "    symbols = []\n",
      "    stack = []\n",
      "    expected_ends = []\n",
      "    \n",
      "    def emit(type, text=None, children=()):\n",
      "        nonlocal pieces\n",
      "        if children is None:\n",
      "            children = pieces\n",
      "            pieces = []\n",
      "        symbols.append(Symbol(type, text, children))\n",
      "    \n",
      "    def expect(*types, save=False):\n",
      "        nonlocal i\n",
      "        if token.type not in types:\n",
      "            type_names = ', '.join(TOKEN_NAMES[t] for t in types)\n",
      "            raise SyntaxError(\n",
      "                'Unexpected %r, expected one of: %r' % (token, type_names))\n",
      "        if save:\n",
      "            pieces.append(token)\n",
      "        i += 1\n",
      "        return True\n",
      "    \n",
      "    def accept(*types, save=False):\n",
      "        nonlocal i\n",
      "        if token.type in types:\n",
      "            if save:\n",
      "                pieces.append(token)\n",
      "            i += 1\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "    def push(expected_end):\n",
      "        nonlocal symbols\n",
      "        stack.append(symbols)\n",
      "        expected_ends.append(expected_end)\n",
      "        symbols = []\n",
      "\n",
      "    def pop():\n",
      "        nonlocal symbols\n",
      "        expected_ends.pop()\n",
      "        group_symbols = symbols\n",
      "        symbols = stack.pop()\n",
      "        return group_symbols\n",
      "\n",
      "    def expected_end():\n",
      "        if expected_ends:\n",
      "            return expected_ends[-1]\n",
      "    \n",
      "    while i < len(tokens):\n",
      "        token = tokens[i]\n",
      "        if accept(T_STRING):\n",
      "            emit(S_STRING, text=token.text)\n",
      "        elif accept(T_LPAREN):\n",
      "            push(T_RPAREN)\n",
      "        elif accept(expected_end()):\n",
      "            emit(S_GROUP, children=pop())\n",
      "        else:\n",
      "            if expected_ends:\n",
      "                msg = 'Unexpected %r, expected %s' % (\n",
      "                    token, TOKEN_NAMES[expected_end()])\n",
      "            elif accept(T_EOF):\n",
      "                continue\n",
      "            else:\n",
      "                msg = 'Unexpected %r' % (token,)\n",
      "            raise SyntaxError(msg)\n",
      "    \n",
      "    expression = RegularExpression(s, symbols)\n",
      "    _cache[s] = expression\n",
      "    return expression\n",
      "\n",
      "\n",
      "def match(pattern, string):\n",
      "    return compile(pattern).match(string)\n",
      "\n",
      "\n",
      "def search(pattern, string):\n",
      "    return compile(pattern).search(string)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert match('test', 'test').groups() == ('test',)\n",
      "#assert match('(test)', 'test').groups() == ('test', 'test')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Realizations\n",
      "\n",
      "I realized a couple days ago that:\n",
      "\n",
      "1. Regular expressions don't have strings, only characters (I can't glob them into strings like I was doing -- `'abc{2}'` matches `'abcc'`, not `'abcabc'`)\n",
      "2. Matching cannot be so involved. What I've gotta do is compile an expression into a structure that makes matching incredibly simple. I'm thinking of having a sort of web, with symbols pointing to the next symbol to match. I think that's what \"finite state automata\" is referring to (*FYI, I haven't done any research on this subject. I wanted to see if I could do it myself.*)\n",
      "\n",
      "I was also struggling to think of a clever way to match wildcard stuff, e.g. `'.*a'`. If I'd finished the above code, it would be compiled to: `[Repeat(AllChars(), float('Inf')), Chars('a')]`. Given the string `\"123a\"`, if I matched each symbol to its fullest, the `'.*'` would match the entire string, and the `'a'` part of the pattern would find the EOF. Matching linearly just won't cut it.\n",
      "\n",
      "I thought today that I could let the wildcard gobble up everything, then afterward search through what had been gobbled with `'a'`."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}